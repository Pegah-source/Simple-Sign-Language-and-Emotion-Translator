{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CombinedExecutionWhile (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from utils.image_classifier import ImageClassifier, NO_FACE_LABEL\n","\n","import cv2.cv2 as cv2\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","import os\n","import sqlite3\n","from keras.models import load_model\n","from clahe import clahe\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","prediction = None\n","\n","\n","\n","#trained model for sign-language detection\n","\n","model = load_model('C:/Users/Yassine/Downloads/inf573/cnn_model_keras2.h5')\n","\n","# Color RGB Codes & Font\n","\n","WHITE_COLOR = (255, 255, 255)\n","GREEN_COLOR = (0, 255, 0)\n","BLUE_COLOR = (255, 255, 104)\n","FONT = cv2.QT_FONT_NORMAL\n","\n","# Frame Width & Height\n","\n","FRAME_WIDTH = 640\n","FRAME_HEIGHT = 490\n","\n","#link to camera feed (set to uri = '0' for webcam)\n","uri = '0'\n","\n","def emote(label):\n","    if label == \"no face\":\n","        return (255,255,255), \"has no face\"\n","    elif label == \"neutral\":\n","        return (255,255,255), \"is bored\"\n","    elif label == \"anger\":\n","        return (0,0,255), \"is angry\"\n","    elif label == \"happy\":\n","        return (0,100,255), \"is happy\"\n","    elif label == \"surprise\":\n","        return (255,255,0), \"is surprised\"\n","    elif label == \"sadness\":\n","        return (255,0,0), \"is sad\"\n","    elif label == \"disgust\":\n","        return (0,255,0), \"is disgusted\"\n","    elif label == \"fear\":\n","        return (255,0,100), \"is scared\"\n","    return (255,255,255), \"none\"\n","    \n","\n","\n","def get_image_size():\n","\timg = cv2.imread('C:/Users/Yassine/Downloads/inf573/gestures/1/100.jpg', 0)\n","\treturn img.shape\n","\n","image_x, image_y = (50,50)\n","\n","def get_hand_hist():\n","\twith open(\"C:/Users/Yassine/Downloads/inf573/hist\", \"rb\") as f:\n","\t\thist = pickle.load(f)\n","\treturn hist\n","\n","#sign language prediction helpers\n","\n","def keras_process_image(img):\n","\timg = cv2.resize(img, (image_x, image_y))\n","\timg = np.array(img, dtype=np.float32)\n","\timg = np.reshape(img, (1, image_x, image_y, 1))\n","\treturn img\n","\n","def keras_predict(model, image):\n","\tprocessed = keras_process_image(image)\n","\tpred_probab = model.predict(processed)[0]\n","\tpred_class = list(pred_probab).index(max(pred_probab))\n","\treturn max(pred_probab), pred_class\n","\n","#text helper functions\n","\n","def get_pred_text_from_db(pred_class):\n","\tconn = sqlite3.connect(\"C:/Users/Yassine/Downloads/inf573/gesture_db.db\")\n","\tcmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class)\n","\tcursor = conn.execute(cmd)\n","\tfor row in cursor:\n","\t\treturn row[0]\n","\n","\n","\t\n","def split_sentence(text, num_of_words):\n","\t\n","\tlist_words = text.split(\" \")\n","\tlength = len(list_words)\n","  # making patches of words of length num_of_words\n","\tsplitted_sentence = []\n","\tb_index = 0\n","\te_index = num_of_words\n","\twhile length > 0:\n","\t\tpart = \"\"\n","\t\tfor word in list_words[b_index:e_index]:\n","\t\t\tpart = part + \" \" + word\n","\t\tsplitted_sentence.append(part)\n","\t\tb_index += num_of_words\n","\t\te_index += num_of_words\n","\t\tlength -= num_of_words\n","\treturn splitted_sentence\n","\n","def put_splitted_text_in_blackboard(blackboard, splitted_text, color):\n","\ty = 200\n","\tfor text in splitted_text:\n","\t\tcv2.putText(blackboard, text, (4, y), cv2.FONT_HERSHEY_TRIPLEX, 2, color)\n","\t\ty += 50\n","\n","\n","\n","class BoundingBox:\n","  def __init__(self, x, y, w, h):\n","    self.x = x\n","    self.y = y\n","    self.w = w\n","    self.h = h\n","\n","  @property\n","  def origin(self) -> tuple:\n","    return self.x, self.y\n","  @property\n","  def top_right(self) -> int:\n","    return self.x + self.w\n","  @property\n","  def bottom_left(self) -> int:\n","    return self.y + self.h\n","\n","# given an image and its bounding bow we make a rectangle that is added to the image\n","def draw_face_rectangle(bb: BoundingBox, img, color=BLUE_COLOR):\n","    cv2.rectangle(img, bb.origin, (bb.top_right, bb.bottom_left), color, 2)\n","\n","# \n","def draw_landmark_points(points: np.ndarray, img, color=WHITE_COLOR):\n","  if points is None:\n","    return None\n","  for (x, y) in points:\n","    cv2.circle(img, (x, y), 1, color, -1)\n","\n","# what is label.upper\n","def write_label(x: int, y: int, label: str, img, color=BLUE_COLOR):\n","  if label == NO_FACE_LABEL:\n","    cv2.putText(img, label.upper(), (int(FRAME_WIDTH / 2), int(FRAME_HEIGHT / 2)), FONT, 1, color, 2, cv2.LINE_AA)\n","  cv2.putText(img, label, (x + 10, y - 10), FONT, 1, color, 2, cv2.LINE_AA)\n","\n","# Contrast Limited Adaptive Histogram Equalization\n","class RealTimeEmotionDetector:\n","  #CLAHE = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","  vidCapture = None\n","  def __init__(self, classifier_model: ImageClassifier):\n","    self.__init_video_capture(frame_w=FRAME_WIDTH, frame_h=FRAME_HEIGHT)\n","    self.classifier = classifier_model\n","  \n","  def __init_video_capture(self, frame_w: int, frame_h: int):\n","    self.vidCapture = cv2.VideoCapture(0)\n","    self.vidCapture.set(cv2.CAP_PROP_FRAME_WIDTH, frame_w)\n","    self.vidCapture.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_h)\n","    \n","  def read_frame(self) -> np.ndarray:\n","    rect, frame = self.vidCapture.read()\n","    return frame\n","  \n","  \n","  def transform_img(self, img: np.ndarray) -> np.ndarray:\n","    # load the input image, resize it, and convert it to gray-scale\n","    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # convert to gray-scale\n","    resized_img = clahe(gray_img,8)  # resize\n","    print(type(resized_img))\n","    return resized_img\n","\n","  def recognize(self):\n","    global prediction\n","    cam = cv2.VideoCapture(0)\n","    if cam.read()[0] == False:\n","      cam = cv2.VideoCapture(0)\n","    hist = get_hand_hist()\n","    x, y, w, h = 300, 100, 300, 300\n","    # emotion **\n","    frame_cnt = 0\n","    predicted_labels = ['']\n","    old_txt = None\n","    rectangles = [(0, 0, 0, 0)]\n","    landmark_points_list = [[(0, 0)]]\n","    wait_key_delay=33\n","    quit_key='q'\n","    frame_period_s=0.75\n","    # emotion **\n","    text = \"\"\n","    \n","    #main loop\n","    \n","    while True:\n","      im = self.read_frame()\n","      im = cv2.flip(im, 1)\n","      frame_cnt += 1\n","      \n","      #emotion prediction every fixed number of frames    \n","      \n","      if frame_cnt % (frame_period_s * 100) == 0:\n","        predicted_labels = self.classifier.classify(img=self.transform_img(img=im))\n","        rectangles = self.classifier.extract_face_rectangle(img=im)\n","        landmark_points_list = self.classifier.extract_landmark_points(img=im)\n","        \n","      # classifier is a class written in another piece of code\n","      \n","      for lbl, rectangle, lm_points in zip(predicted_labels, rectangles, landmark_points_list):\n","        draw_face_rectangle(BoundingBox(*rectangle), im)\n","        draw_landmark_points(points=lm_points, img=im)\n","        #write_label(rectangle[0], rectangle[1], label=lbl, img=im)\n","        if old_txt != predicted_labels:\n","        \n","          # if the emotion has got changed\n","          \n","          print('[INFO] Predicted Labels:', predicted_labels)\n","          old_txt = predicted_labels\n","          \n","      # sign part\n","      \n","      im = cv2.resize(im, (640, 480))\n","      imgCrop = im[y:y+h, x:x+w]\n","      imgHSV = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","      \n","      # Back project to get hand detection\n","      \n","      dst = cv2.calcBackProject([imgHSV], [0, 1], hist, [0, 180, 0, 256], 1)\n","      disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n","      cv2.filter2D(dst,-1,disc,dst)\n","      blur = cv2.GaussianBlur(dst, (11,11), 0)\n","      blur = cv2.medianBlur(blur, 15)\n","      thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n","      thresh = cv2.merge((thresh,thresh,thresh))\n","      thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n","      thresh = thresh[y:y+h, x:x+w]\n","      (openCV_ver,_,__) = cv2.__version__.split(\".\")\n","      if openCV_ver=='3':\n","        contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[1]\n","      elif openCV_ver=='4':\n","        contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n","      if len(contours) > 0:\n","        contour = max(contours, key = cv2.contourArea)\n","        #print(cv2.contourArea(contour))\n","        if cv2.contourArea(contour) > 10000:\n","          x1, y1, w1, h1 = cv2.boundingRect(contour)\n","          save_img = thresh[y1:y1+h1, x1:x1+w1]\n","          \n","          if w1 > h1:\n","            save_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n","          elif h1 > w1:\n","            save_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n","            \n","          pred_probab, pred_class = keras_predict(model, save_img)\n","          if pred_probab*100 > 80:\n","            text = get_pred_text_from_db(pred_class)\n","            print(text)\n","\n","      col, lab = emote(predicted_labels[0])      \n","      blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n","      splitted_text = split_sentence(text + \" \" + lab, 2)\n","      put_splitted_text_in_blackboard(blackboard, splitted_text,col)\n","      #cv2.putText(blackboard, text, (30, 200), cv2.FONT_HERSHEY_TRIPLEX, 1.3, (255, 255, 255))\n","      cv2.rectangle(im, (x,y), (x+w, y+h), (0,255,0), 2)\n","      res = np.hstack((im, blackboard))\n","      cv2.imshow(\"Recognizing gesture\", res)\n","      cv2.imshow(\"thresh\", thresh)\n","      if cv2.waitKey(1) == ord('q'):\n","        break\n","\n","# driver function\n","\n","def run_real_time_emotion_detector(\n","        classifier_algorithm: str,\n","        predictor_path: str,\n","        dataset_csv: str,\n","        dataset_images_dir: str = None):\n","  from utils.data_land_marker import LandMarker\n","  from utils.image_classifier import ImageClassifier\n","  from os.path import isfile\n","  \n","  land_marker = LandMarker(landmark_predictor_path=predictor_path)\n","\n","  if not isfile(dataset_csv): \n","    # If data-set not built before.\n","    print('[INFO]', f'Dataset file: \"{dataset_csv}\" could not found.')\n","    from data_preparer import run_data_preparer\n","    run_data_preparer(land_marker, dataset_images_dir, dataset_csv)\n","  else:\n","    print('[INFO]', f'Dataset file: \"{dataset_csv}\" found.')\n","    \n","  classifier = ImageClassifier(csv_path=dataset_csv, algorithm=classifier_algorithm, land_marker=land_marker)\n","  print('[INFO] Opening camera, press \"q\" to exit..')\n","  RealTimeEmotionDetector(classifier_model=classifier).recognize()\n","\n","\n","if __name__ == \"__main__\":\n","  \"\"\"The value of the parameters can change depending on the case.\"\"\"\n","  #keras_predict(model, np.zeros((50, 50), dtype=np.uint8))\n","  run_real_time_emotion_detector(classifier_algorithm='RandomForest',predictor_path='utils/shape_predictor_68_face_landmarks.dat',dataset_csv='data/csv/dataset.csv',dataset_images_dir='data/raw')\n","  print('Successfully terminated.')"],"metadata":{"id":"5s3oCbt_nlSp"},"execution_count":null,"outputs":[]}]}